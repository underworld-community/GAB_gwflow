{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAB model: groundwater + heat flow\n",
    "\n",
    "Each layer corresponds to the region between two surfaces as defined in `UW-GAB_layer_parameters.csv`. Each layer is assigned a \"material index\" which is used to map the thermal and hydraulic properties:\n",
    "\n",
    "- $k_h$: hydraulic conductivity (m/s)\n",
    "- $\\phi$: porosity\n",
    "- $k_T$: thermal conductivity (W/m/K)\n",
    "- $H$: rate of heat production (W/m$^3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underworld as uw\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "from scipy.interpolate import RegularGridInterpolator, LinearNDInterpolator\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy import optimize\n",
    "\n",
    "import underworld.visualisation as vis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from time import time\n",
    "\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "\n",
    "### convert between units\n",
    "from pint import UnitRegistry\n",
    "u = UnitRegistry()\n",
    "\n",
    "### additional requirements, may need to be installed\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import pyvista as pv\n",
    "\n",
    "\n",
    "\n",
    "startTiming   = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPC_run check to change between resolutions and directories quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydraulicConductivityOnly = True\n",
    "\n",
    "if uw.mpi.size == 1:\n",
    "    HPC_run = False\n",
    "else:\n",
    "    HPC_run = True\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set key parameters for the MH algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Initial burn-in period\n",
    "BURNIN = 50\n",
    "\n",
    "### number of simulations\n",
    "NSIM   = 500\n",
    "#### Tempering required for models due to high misfit\n",
    "TEMPERING = 10e3\n",
    "\n",
    "### sigma to be used by the model, however MH includes an adaptive sigma to keep acceptance rate between 20 and 50 %\n",
    "sigma  = 1.\n",
    "\n",
    "### adapt sigma every nth interval to keep acceptance rate between 20 and 50 %\n",
    "tune_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set key parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### will write to the output file\n",
    "verbose = True\n",
    "\n",
    "### set to 1 or 'True' to store model outputs\n",
    "n_checkpoints = 1\n",
    "\n",
    "###### Gauss point count, produces a total particles per cell of GPC ** model dimensions\n",
    "#### Gauss point count of up to 4 seems to prevent the model from crashing.\n",
    "if HPC_run == True:\n",
    "    ### used for initial material distribution on the swarm\n",
    "    GPC = 4\n",
    "    ### used for swarm that does the solve\n",
    "    GPC_solver = 2\n",
    "else:\n",
    "    ### used for initial material distribution on the swarm\n",
    "    GPC = 2\n",
    "    ### used for swarm that does the solve\n",
    "    GPC_solver = 2\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets\n",
    "\n",
    "###### Directories where data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### directory of stored data\n",
    "if HPC_run == True:\n",
    "    data_dir = '/home/565/bk2562/models/GAB_data/data/'\n",
    "else:\n",
    "    data_dir = '/home/jovyan/workspace/GAB-Notebooks/data/'\n",
    "\n",
    "numpy_directory = data_dir + \"GAB_surfaces/NumPy/\"\n",
    "\n",
    "geotiff_directory = data_dir + \"GAB_surfaces/GeoTiff/\"\n",
    "# png_directory = \"../data/GAB_surfaces/png/\"\n",
    "\n",
    "\n",
    "surface_filename_npz = numpy_directory + \"{:s}.npz\"\n",
    "surface_filename_tiff = geotiff_directory + \"{:s}.tiff\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model dimensions and parameters\n",
    "\n",
    "Change the xmin, xmax etc values to the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmin = 298.0\n",
    "Tmax = 500.0\n",
    "Nx, Ny, Nz = 20,20,50 # global size\n",
    "\n",
    "# define bounding box\n",
    "xmin, xmax, ymin, ymax = -955637.8812, 1034362.2443650428, 6342298.2975, 8922298.39436168\n",
    "zmin, zmax = -8000.0, 1200.0\n",
    "\n",
    "# resolution\n",
    "if HPC_run == True:\n",
    "    dx, dy, dz = 10e3, 10e3, 100\n",
    "else:\n",
    "    dx, dy, dz = 40e3, 40e3, 1e3\n",
    "# dx, dy, dz = 10e3, 10e3, 50\n",
    "Nx, Ny, Nz = int((xmax-xmin)/dx), int((ymax-ymin)/dy), int((zmax-zmin)/dz)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if uw.mpi.rank == 0:\n",
    "    print(f'Particles per cell: {GPC ** 3}')\n",
    "\n",
    "    print(\"global number of elements in x,y,z {} | total number of elements = {}\".format((Nx,Ny,Nz), Nx*Ny*Nz))\n",
    "\n",
    "    print(f'Total particles: {Nx*Ny*Nz*(GPC**3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file directory to store model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HPC_run == True:\n",
    "    simulation_directory = \"MHModel-{}TuneInterval_newkh0-{}Burnin_{}Nsims-{}Tempering_{}dx-{}dy-{}dz_\".format(tune_interval, BURNIN, NSIM, TEMPERING, dx/1e3, dy/1e3, dz/1e3) + \"{}PPC/\".format(GPC_solver**3)\n",
    "else:\n",
    "    simulation_directory = \"MHModel-{}TuneInterval_newkh0-{}Burnin_{}Nsims-{}Tempering_{}dx-{}dy-{}dz_\".format(tune_interval, BURNIN, NSIM, TEMPERING, dx/1e3, dy/1e3, dz/1e3) + \"{}PPC/\".format(GPC_solver**3)\n",
    "\n",
    "if uw.mpi.rank == 0:\n",
    "    if not os.path.exists(simulation_directory):\n",
    "        os.makedirs(simulation_directory)\n",
    "\n",
    "\n",
    "if uw.mpi.rank == 0 and n_checkpoints == 1:\n",
    "    if not os.path.exists(simulation_directory+'checkpoints/'):\n",
    "        os.makedirs(simulation_directory+'checkpoints/')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_layers contains key information of the model in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layers = pd.read_csv(data_dir + \"GAB_surfaces/UW-GAB_layer_parameters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numpy arrays of datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key info stored in the csv file:\n",
    "- kh0 = hydraulic conductivity\n",
    "- kt0 = thermal conductivity\n",
    "- a = \n",
    "- H = heat production\n",
    "- porisityData = Porosity\n",
    "- LayerNames = names of lithological layers in the model\n",
    "\n",
    "These can either be from the dataframe, or can be passed as numpy arrays/lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matIndex = np.int32(df_layers['mat index'].iloc[1:-1].values)\n",
    "\n",
    "## take average in log 10 space\n",
    "kh0_log = (np.log10(df_layers['ogia conductivity min (m/day)'].iloc[1:-1].values) + np.log10(df_layers['ogia conductivity max (m/day)'].iloc[1:-1].values)) / 2.\n",
    "\n",
    "## convert from log space and then convert from m/day to m/second\n",
    "kh0 = ((10**kh0_log) * u.meter/u.day).to(u.meter/u.second).magnitude\n",
    "\n",
    "\n",
    "\n",
    "kt0 = df_layers['thermal conductivity'].iloc[1:-1].values\n",
    "\n",
    "dkt = 10*kt0\n",
    "\n",
    "a = df_layers['a (T)'].iloc[1:-1].values\n",
    "\n",
    "\n",
    "H0 = df_layers['Heat production (W/m3)'].iloc[1:-1].values\n",
    "\n",
    "dH = 10*H0\n",
    "\n",
    "porosityData = (df_layers['Porosity (%Vol)'].iloc[1:-1].values) / 100\n",
    "\n",
    "LayerNames = list(df_layers['Layer name'].iloc[1:].values)\n",
    "\n",
    "# dH = df_layers['Heat production error'][1:-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initialise a Q1 finite element mesh and mesh variables.\n",
    "\n",
    "deformedmesh = True\n",
    "elementType = \"Q1\"\n",
    "mesh = uw.mesh.FeMesh_Cartesian( elementType = (elementType),\n",
    "                                 elementRes  = (Nx,Ny,Nz),\n",
    "                                 minCoord    = (xmin,ymin,zmin),\n",
    "                                 maxCoord    = (xmax,ymax,zmax))\n",
    "\n",
    "gwHydraulicHead            = mesh.add_variable( nodeDofCount=1 )\n",
    "temperatureField           = mesh.add_variable( nodeDofCount=1 )\n",
    "temperatureField0          = mesh.add_variable( nodeDofCount=1 )\n",
    "velocityField              = mesh.add_variable( nodeDofCount=3 )\n",
    "heatProductionField        = mesh.add_variable( nodeDofCount=1 )\n",
    "\n",
    "materialMesh               = mesh.add_variable( nodeDofCount=1 )\n",
    "\n",
    "### used on the velocity field to caculate darcy flow\n",
    "porosity                = mesh.add_variable( nodeDofCount=1 )\n",
    "\n",
    "\n",
    "coords = mesh.data\n",
    "\n",
    "Xcoords = np.unique(coords[:,0])\n",
    "Ycoords = np.unique(coords[:,1])\n",
    "Zcoords = np.unique(coords[:,2])\n",
    "nx, ny, nz = Xcoords.size, Ycoords.size, Zcoords.size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deform mesh to surface & basement surfaces\n",
    "\n",
    "We want to deform the $z$-axis spacing so that the surface of the mesh is draped over the topography at the top and the basement rocks at the base.\n",
    "\n",
    "\n",
    "\n",
    "The topography (topo_interp) and basement (basement_interp) filenames will have to be changed when changing the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_interp = None\n",
    "basement_interp = None\n",
    "\n",
    "\n",
    "if uw.mpi.rank == 0:\n",
    "\n",
    "\n",
    "    with np.load(surface_filename_npz.format(\"AUSBATH09_AMG55_GDA94_500m_model_extent\")) as npz:\n",
    "        topo_interp = RegularGridInterpolator((npz['y'], npz['x']), np.flipud(npz['data']))\n",
    "\n",
    "#     with np.load(surface_filename_npz.format(\"W910_BASEMENT_v2\")) as npz:\n",
    "#         basement_interp = RegularGridInterpolator((npz['y'], npz['x']), np.flipud(npz['data']))\n",
    "\n",
    "    with rioxarray.open_rasterio(surface_filename_tiff.format(\"W910_BASEMENT_v2\")) as npz:\n",
    "        basement_interp = RegularGridInterpolator((np.flipud(npz.sel(band=1).y.data), npz.sel(band=1).x.data), np.flipud(npz.sel(band=1).data), bounds_error=False)\n",
    "\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "topo_interp = uw.mpi.comm.bcast(topo_interp, root=0)\n",
    "basement_interp = uw.mpi.comm.bcast(basement_interp, root=0)\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "\n",
    "local_topography = topo_interp((mesh.data[:,1], mesh.data[:,0]))\n",
    "local_basement = basement_interp((mesh.data[:,1], mesh.data[:,0]))\n",
    "\n",
    "local_basement[np.isnan(local_basement)] = 0.\n",
    "\n",
    "# ensure basement is at least as deep as topography!\n",
    "local_basement = np.minimum(local_basement, local_topography)\n",
    "\n",
    "\n",
    "\n",
    "# subtract a thickness buffer\n",
    "dz_min = 2e3\n",
    "local_basement -= dz_min\n",
    "\n",
    "with mesh.deform_mesh():\n",
    "    zcube = coords[:,2].reshape(nz,ny,nx)\n",
    "    zcube_norm = zcube.copy()\n",
    "    zcube_norm -= zmin\n",
    "    zcube_norm /= zmax - zmin\n",
    "    # difference to add to existing z coordinates\n",
    "    dzcube1 = zcube_norm * -(zmax - local_topography.reshape(zcube.shape))\n",
    "    dzcube0 = (1.0 - zcube_norm) * -(zmin - local_basement.reshape(zcube.shape))\n",
    "\n",
    "    mesh.data[:,2] += dzcube1.ravel()\n",
    "    mesh.data[:,2] += dzcube0.ravel()\n",
    "    coords = mesh.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the types of boundary conditions\n",
    "\n",
    "- A Neumann boundary condition is applied to the side and bottom boundaries, which allows inflow and outflow but the total flux is 0.\n",
    "- The top pressure boundary condition is set to the smoothed topopgraphic surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "topWall = mesh.specialSets[\"MaxK_VertexSet\"]\n",
    "bottomWall = mesh.specialSets[\"MinK_VertexSet\"]\n",
    "\n",
    "closeWall = mesh.specialSets[\"MinJ_VertexSet\"]\n",
    "farWall    = mesh.specialSets[\"MaxJ_VertexSet\"]\n",
    "\n",
    "leftWall   = mesh.specialSets[\"MinI_VertexSet\"]\n",
    "rightWall  = mesh.specialSets[\"MaxI_VertexSet\"]\n",
    "\n",
    "sideWalls = closeWall + farWall + leftWall + rightWall\n",
    "\n",
    "gwPressureBC = uw.conditions.DirichletCondition( variable      = gwHydraulicHead,\n",
    "                                               indexSetsPerDof = ( topWall   ) )\n",
    "\n",
    "temperatureBC = uw.conditions.DirichletCondition( variable        = temperatureField,\n",
    "                                                  indexSetsPerDof = (topWall+bottomWall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwHydraulicHead.data[:] = 0.\n",
    "\n",
    "# # create a linear gradient [0, 1] top to bottom of mesh\n",
    "znorm = mesh.data[:,2].copy()\n",
    "znorm -= zmin\n",
    "znorm /= (zmax-zmin)\n",
    "# znorm = (zmin - local_topography)/zmin\n",
    "linear_gradient = 1.0 - znorm\n",
    "\n",
    "zmax - local_topography\n",
    "\n",
    "# pressure and temperature initial conditions\n",
    "initial_pressure = linear_gradient*(zmax-zmin)\n",
    "initial_temperature = linear_gradient*(Tmax - Tmin) + Tmin\n",
    "initial_temperature = np.clip(initial_temperature, Tmin, Tmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwHydraulicHead.data[:]  = initial_pressure.reshape(-1,1)\n",
    "temperatureField.data[:] = initial_temperature.reshape(-1,1)\n",
    "\n",
    "# assign BCs (account for pressure of water below sea level)\n",
    "sealevel = 0.0\n",
    "seafloor = topWall[mesh.data[topWall,2] < sealevel]\n",
    "\n",
    "gwHydraulicHead.data[topWall] = 0.\n",
    "gwHydraulicHead.data[seafloor] = -((mesh.data[seafloor,2]-sealevel)*1.0).reshape(-1,1)\n",
    "temperatureField.data[topWall] = Tmin\n",
    "temperatureField.data[bottomWall] = Tmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import water table surface\n",
    "\n",
    "The water table surface file name and directory will need to be changed when modelling a different area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rgi_wt = None\n",
    "\n",
    "if uw.mpi.rank == 0:\n",
    "    with np.load(data_dir+ \"GAB_surfaces/\"+\"water_table_surface.npz\") as npz:\n",
    "        wt = npz['data']\n",
    "        wt_x = npz['x']\n",
    "        wt_y = npz['y']\n",
    "\n",
    "    rgi_wt = RegularGridInterpolator((wt_y, wt_x), wt)\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "rgi_wt = uw.mpi.comm.bcast(rgi_wt, root=0)\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "\n",
    "\n",
    "wt_interp = rgi_wt(mesh.data[topWall,0:2][:,::-1])\n",
    "# gwHydraulicHead.data[topWall] = (-wt_interp * 1000.0 * 9.81).reshape(-1,1)\n",
    "\n",
    "zCoordFn = uw.function.input()[2]\n",
    "yCoordFn = uw.function.input()[1]\n",
    "xCoordFn = uw.function.input()[0]\n",
    "\n",
    "gwHydraulicHead.data[:] = zCoordFn.evaluate(mesh)\n",
    "gwHydraulicHead.data[topWall] += (-wt_interp).reshape(-1,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up particle swarm which will store the material properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High initial PPC swarm to load the layers in a high resolution\n",
    "\n",
    "swarm0         = uw.swarm.Swarm( mesh=mesh )\n",
    "swarmLayout0   = uw.swarm.layouts.PerCellGaussLayout(swarm=swarm0,gaussPointCount=GPC)\n",
    "swarm0.populate_using_layout( layout=swarmLayout0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "materialIndex0           = swarm0.add_variable( dataType=\"int\",    count=1 )\n",
    "hydraulicDiffusivity0    = swarm0.add_variable( dataType=\"double\", count=1 )\n",
    "thermalDiffusivity0      = swarm0.add_variable( dataType=\"double\", count=1 )\n",
    "heatProduction0          = swarm0.add_variable( dataType=\"double\", count=1 )\n",
    "a_exponent0              = swarm0.add_variable( dataType=\"double\", count=1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up particle swarm for solver\n",
    "\n",
    "Each cell contains particles that _must_ be assigned isotropic thermal and hydraulic properties.\n",
    "\n",
    "> A Gauss Point Count (GPC) of __Four__ maximum (64 particles per cell) seems to prevent the model from crashing.\n",
    ">> This is asigned through the GPC_solver variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "swarm         = uw.swarm.Swarm( mesh=mesh )\n",
    "swarmLayout   = uw.swarm.layouts.PerCellGaussLayout(swarm=swarm, gaussPointCount=GPC_solver)\n",
    "swarm.populate_using_layout( layout=swarmLayout )\n",
    "\n",
    "\n",
    "swarmVelocity           = swarm.add_variable( dataType=\"double\", count=3 )\n",
    "\n",
    "materialIndex           = swarm.add_variable( dataType=\"int\",    count=1 )\n",
    "fn_hydraulicDiffusivity = swarm.add_variable( dataType=\"double\", count=1 )\n",
    "\n",
    "thermalDiffusivity      = swarm.add_variable( dataType=\"double\", count=1 )\n",
    "heatProduction          = swarm.add_variable( dataType=\"double\", count=1 )\n",
    "a_exponent              = swarm.add_variable( dataType=\"double\", count=1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import geological surfaces\n",
    "\n",
    "- Assign a material index for particles that lie below a surface.\n",
    "- Is a 'layered' model, where all material below the surface is updated. Therefore the layer ordering _must_ be from top to bottom.\n",
    "\n",
    "## Assign material properties\n",
    "\n",
    "Use level sets to assign hydraulic diffusivities to a region on the mesh corresponding to any given material index.\n",
    "\n",
    "- $H$       : rate of heat production\n",
    "- $\\rho$     : density\n",
    "- $k_h$     : hydraulic conductivity\n",
    "- $k_t$     : thermal conductivity\n",
    "- $\\kappa_h$ : hydraulic diffusivity\n",
    "- $\\kappa_t$ : thermal diffusivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set all material to top layer initially\n",
    "index = matIndex[0]\n",
    "\n",
    "### assign material variables\n",
    "materialIndex0.data[:] = index\n",
    "\n",
    "materialMesh.data[:] = index\n",
    "\n",
    "#### update all sidewalls to basement material\n",
    "materialMesh.data[sideWalls] = matIndex[-1]\n",
    "\n",
    "materialIndex.data[:] = index\n",
    "\n",
    "### add in layer properties\n",
    "hydraulicDiffusivity0.data[:] = kh0[index] #((((df_layers['ogia conductivity max (m/day)'].iloc[index].values,df_layers['ogia conductivity min (m/day)'].iloc[index].values)/2.)  * u.meter/u.day).to(u.meter/u.second).magnitude)\n",
    "\n",
    "\n",
    "thermalDiffusivity0.data[:] = kt0[index]#df_layers['thermal conductivity'].iloc[index]\n",
    "\n",
    "a_exponent0.data[:] = a[index]#df_layers['a (T)'].iloc[index]\n",
    "\n",
    "heatProduction0.data[:] = H0[index] #(df_layers['Heat production (W/m3)'].iloc[index])\n",
    "\n",
    "\n",
    "porosity.data[:] = porosityData[index] #(df_layers['Porosity (%Vol)'].iloc[index]) / 100 # from % to a value between 0 and 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Cell centroid method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code will have to be modified when modelling a different area:\n",
    "\n",
    "'row = df_layers.loc[df_layers['mat index'] == index]'\n",
    "\n",
    "To where layer names are stored to load in the correct files for that layers surface\n",
    "\n",
    "\n",
    "The block below loads in the surfaces from top to bottom and updates the material properties accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_layer = np.ones(swarm.data.shape[0], dtype=bool)\n",
    "\n",
    "\n",
    "## starting from the surface and going deeper with each layer\n",
    "for index in matIndex:\n",
    "    row = df_layers.loc[df_layers['mat index'] == index]\n",
    "\n",
    "    # load surface\n",
    "    layer_interp = None\n",
    "    if uw.mpi.rank == 0:\n",
    "\n",
    "        with rioxarray.open_rasterio(surface_filename_tiff.format(row['Layer name'].iloc[0])) as npz:\n",
    "            layer_interp = RegularGridInterpolator((np.flipud(npz.sel(band=1).y.data), npz.sel(band=1).x.data), np.flipud(npz.sel(band=1).data), bounds_error=False)\n",
    "\n",
    "\n",
    "    uw.mpi.comm.barrier()\n",
    "\n",
    "    layer_interp = uw.mpi.comm.bcast(layer_interp, root=0)\n",
    "\n",
    "    uw.mpi.comm.barrier()\n",
    "\n",
    "#     ### interpolate surface to high res swarm0\n",
    "    z_interp0 = layer_interp((swarm0.data[:,1], swarm0.data[:,0]))\n",
    "    ### interpolate surface to high low res swarm\n",
    "    z_interp = layer_interp((swarm.data[:,1], swarm.data[:,0]))\n",
    "    ### interpolate surface to mesh\n",
    "    z_interp_mesh = layer_interp((mesh.data[:,1], mesh.data[:,0]))\n",
    "\n",
    "###     assign index to swarm particles which are below the current surface\n",
    "###     if they are above the surface then we are done.\n",
    "\n",
    "\n",
    "    mask_layer0     = swarm0.data[:,2] < z_interp0\n",
    "\n",
    "    mask_layer      = swarm.data[:,2] < z_interp\n",
    "\n",
    "    mask_layer_mesh = mesh.data[:,2] < z_interp_mesh\n",
    "\n",
    "    ### assign material variables\n",
    "    materialIndex0.data[mask_layer0] = index\n",
    "\n",
    "    materialIndex.data[mask_layer] = index\n",
    "\n",
    "    materialMesh.data[mask_layer_mesh] = index\n",
    "\n",
    "    \n",
    "    ### add in layer properties\n",
    "    hydraulicDiffusivity0.data[mask_layer0] = kh0[index] #((((df_layers['ogia conductivity max (m/day)'].iloc[index].values,df_layers['ogia conductivity min (m/day)'].iloc[index].values)/2.)  * u.meter/u.day).to(u.meter/u.second).magnitude)\n",
    "\n",
    "\n",
    "    thermalDiffusivity0.data[mask_layer0] = kt0[index]#df_layers['thermal conductivity'].iloc[index]\n",
    "\n",
    "    a_exponent0.data[mask_layer0] = a[index]#df_layers['a (T)'].iloc[index]\n",
    "\n",
    "    heatProduction0.data[mask_layer0] = H0[index] #(df_layers['Heat production (W/m3)'].iloc[index])\n",
    "\n",
    "\n",
    "    porosity.data[mask_layer_mesh] = porosityData[index] #(df_layers['Porosity (%Vol)'].iloc[index]) / 100 # from % to a value between 0 and 1\n",
    "\n",
    "\n",
    "    if uw.mpi.rank == 0:\n",
    "        print(\"Layer {:2d}  {}  ({})\".format(index, row['Name Aquifer/Aquitard'].iloc[0], row['Layer name'].iloc[0]))\n",
    "\n",
    "# determine mean of cells to produce uniform cells on solver swarm\n",
    "fn_hydraulicDiffusivity.data[:,0] = np.repeat((np.mean((np.split(hydraulicDiffusivity0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "\n",
    "thermalDiffusivity.data[:,0] = np.repeat((np.mean((np.split(thermalDiffusivity0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "heatProduction.data[:,0] = np.repeat((np.mean((np.split(heatProduction0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "\n",
    "a_exponent.data[:,0] = np.repeat((np.mean((np.split(a_exponent0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise in PyVista to make sure model has been set up correctly\n",
    "\n",
    "Only do this locally (when on 1 CPU), not on the HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if uw.mpi.size == 1:\n",
    "    ### create a structured grid\n",
    "    grid = pv.StructuredGrid()\n",
    "    ### load in the points of the grid\n",
    "    grid.points = mesh.data[:,]\n",
    "    ### set the dimensions of the grid\n",
    "    grid.dimensions = Nx+1, Ny+1, Nz+1\n",
    "    \n",
    "    grid[\"materialMesh\"] = materialMesh.data[:,0]\n",
    "    \n",
    "    ### any of the mesh properties can be added\n",
    "    \n",
    "    grid[\"porosity\"] = porosity.data[:,0]\n",
    "    \n",
    "    \n",
    "    ### Plot the entire model\n",
    "\n",
    "    p = pv.Plotter()\n",
    "\n",
    "    p.add_mesh(grid, scalars='materialMesh', cmap='Spectral', opacity=1)\n",
    "    \n",
    "    # p.add_mesh(grid, scalars='porosity', cmap='Spectral', opacity=1)\n",
    "    \n",
    "    #### modify camera view\n",
    "\n",
    "    # p.camera_position = 'xy'\n",
    "    # p.camera.view_angle = 20\n",
    "\n",
    "    # p.camera_position = [(182.0, 177.0, 50), (139, 105, 19), (-0.2, -0.2, 1)]\n",
    "    p.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if uw.mpi.size == 1:\n",
    "    #### plot slices\n",
    "    p = pv.Plotter()\n",
    "\n",
    "    slices = grid.slice_along_axis(n=7, axis=\"y\")\n",
    "\n",
    "    p.add_mesh(grid.outline(), color=\"k\")\n",
    "    \n",
    "    p.add_mesh(slices, scalars='materialMesh', cmap='Spectral', opacity=1)\n",
    "    \n",
    "    # p.add_mesh(slices, scalars='porosity', cmap='Spectral', opacity=1)\n",
    "    \n",
    "    ### modify camera view\n",
    "\n",
    "    # p.camera_position = 'xy'\n",
    "    # p.camera.view_angle = 20\n",
    "\n",
    "    # p.camera_position = [(182.0, 177.0, 50), (139, 105, 19), (-0.2, -0.2, 1)]\n",
    "\n",
    "    p.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model properties & solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +\n",
    "swarm_topography = topo_interp((swarm.data[:,1],swarm.data[:,0]))\n",
    "mesh_topography  = local_topography\n",
    "\n",
    "beta = 9.3e-3\n",
    "depth = -1.0*(materialIndex.swarm.data[:,2] - swarm_topography)\n",
    "depth = np.clip(depth, 0.0, zmax-zmin)\n",
    "\n",
    "depth_mesh = -1.0*(mesh.data[:,2] - mesh_topography)\n",
    "depth_mesh = np.clip(depth_mesh, 0.0, zmax-zmin)\n",
    "\n",
    "# +\n",
    "Storage = 1.\n",
    "rho_water = 1000.\n",
    "c_water = 4e3\n",
    "coeff = rho_water*c_water\n",
    "\n",
    "if deformedmesh:\n",
    "    g = uw.function.misc.constant((0.,0.,-1.))\n",
    "else:\n",
    "    g = uw.function.misc.constant((0.,0.,0.))\n",
    "\n",
    "# g = uw.function.misc.constant((0.,0.,0.))\n",
    "\n",
    "gwPressureGrad = gwHydraulicHead.fn_gradient\n",
    "\n",
    "gMapFn = -g*rho_water*Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_thermalDiffusivity = thermalDiffusivity*(298.0/temperatureField)**a_exponent\n",
    "fn_source = uw.function.math.dot(-1.0*coeff*velocityField, temperatureField.fn_gradient) + heatProductionField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up solvers\n",
    "### gw solver\n",
    "gwadvDiff = uw.systems.SteadyStateDarcyFlow(\n",
    "                                            velocityField    = velocityField, \\\n",
    "                                            pressureField    = gwHydraulicHead, \\\n",
    "                                            fn_diffusivity   = fn_hydraulicDiffusivity, \\\n",
    "                                            conditions       = [gwPressureBC], \\\n",
    "                                            fn_bodyforce     = (0.0, 0.0, 0.0), \\\n",
    "                                            voronoi_swarm    = swarm, \\\n",
    "                                            swarmVarVelocity = swarmVelocity)\n",
    "\n",
    "\n",
    "#### heatflow solver\n",
    "heateqn = uw.systems.SteadyStateHeat( temperatureField = temperatureField,                                       fn_diffusivity   = fn_thermalDiffusivity,                                       fn_heating       = heatProduction,                                       conditions       = temperatureBC                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solve the groudwater flow\n",
    "gwsolver = uw.systems.Solver(gwadvDiff)\n",
    "\n",
    "\n",
    "### solve the heat flow\n",
    "heatsolver = uw.systems.Solver(heateqn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find model elevation\n",
    "topWall_xyz = uw.mpi.comm.allgather(mesh.data[topWall])\n",
    "topWall_xyz = np.vstack(topWall_xyz)\n",
    "\n",
    "# create downsampled interpolator for surface topography\n",
    "interp_downsampled = LinearNDInterpolator(topWall_xyz[:,:2], topWall_xyz[:,2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sprinkle_observations(obs_xyz, dz=10.0, return_swarm=False, return_index=False):\n",
    "    \"\"\"\n",
    "    Place observations on top boundary wall of the mesh - or pretty close to... (parallel safe)\n",
    "    \"\"\"\n",
    "    inside_particles_g = np.zeros(obs_xyz.shape[0], dtype=np.int32)\n",
    "\n",
    "    while not inside_particles_g.all():\n",
    "        swarm_well = uw.swarm.Swarm(mesh=mesh, particleEscape=False)\n",
    "        particle_index = swarm_well.add_particles_with_coordinates(obs_xyz)\n",
    "\n",
    "        inside_particles_l = (particle_index >= 0).astype(np.int32)\n",
    "        inside_particles_g.fill(0)\n",
    "\n",
    "        uw.mpi.comm.Allreduce([inside_particles_l, MPI.INT], [inside_particles_g, MPI.INT], op=MPI.SUM)\n",
    "\n",
    "        # print(comm.rank, np.count_nonzero(inside_particles_g == 0))\n",
    "        obs_xyz[inside_particles_g == 0, 2] -= dz\n",
    "\n",
    "    output_tuple = [obs_xyz]\n",
    "\n",
    "    if return_swarm:\n",
    "        output_tuple.append(swarm_well)\n",
    "    if return_index:\n",
    "        output_tuple.append(particle_index)\n",
    "    return output_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_root(vals, particle_index):\n",
    "    \"\"\"\n",
    "    Gather values from all processors to the root processor\n",
    "    \"\"\"\n",
    "    nparticles = len(particle_index)\n",
    "\n",
    "    # initialise with very low numbers\n",
    "    vl = np.full(nparticles, -999999, np.float32)\n",
    "    vg = np.full(nparticles, -999999, np.float32)\n",
    "    vl[particle_index > -1] = vals.ravel()\n",
    "\n",
    "    # finds the max - aka proper value\n",
    "    uw.mpi.comm.Reduce([vl, MPI.FLOAT], [vg, MPI.FLOAT], op=MPI.MAX, root=0)\n",
    "    return vg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert observational datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insert recharge data\n",
    "- The file name, directory and column headings may need to be changed\n",
    "- The units for the unit conversion may also need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recharge_data = None\n",
    "if uw.mpi.rank == 0:\n",
    "    ti = time()\n",
    "\n",
    "    recharge_data = pd.read_csv(data_dir+'rch_fnl2_mmyr.csv')\n",
    "\n",
    "\n",
    "    ### remove data that doesn't fit within the x bounds\n",
    "    recharge_data = recharge_data[(recharge_data['X'] > xmin) & (recharge_data['X'] < xmax)]\n",
    "    ### remove data that doesn't fit within the y bounds\n",
    "    recharge_data = recharge_data[(recharge_data['Y'] > ymin) & (recharge_data['Y'] < ymax)]\n",
    "\n",
    "    ### remove duplicates if there are any\n",
    "    recharge_data = recharge_data.drop_duplicates(subset=['X', 'Y'], keep='first')\n",
    "\n",
    "\n",
    "    # recharge_data = recharge_data[::10]\n",
    "\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "recharge_data = uw.mpi.comm.bcast(recharge_data, root=0)\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "recharge_E = recharge_data['X'].values\n",
    "recharge_N = recharge_data['Y'].values\n",
    "\n",
    "\n",
    "### convert values from mm/yr to m/s\n",
    "recharge_vel = ((recharge_data['RechargeRates'].values * u.millimeter/u.year).to(u.meter/u.second).magnitude)\n",
    "\n",
    "### recharge std based on std of rr increasing as rr increase. Base STD of 5 mm / yr used\n",
    "recharge_vel_std = (recharge_vel / 4.) + ((0.1 * u.millimeter/u.year).to(u.meter/u.second).magnitude)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "recharge_Z = interp_downsampled(np.c_[recharge_E, recharge_N])\n",
    "\n",
    "recharge_xyz = np.c_[recharge_E, recharge_N, recharge_Z]\n",
    "recharge_xyz, swarm_recharge, index_recharge = sprinkle_observations(recharge_xyz, dz=10., return_swarm=True, return_index=True)\n",
    "\n",
    "\n",
    "if uw.mpi.rank == 0 and verbose:\n",
    "    print(\"number of recharge observations = {}\".format(recharge_xyz.shape[0]))\n",
    "    print(f\"Time to import recharge observations: {time()-ti} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load hydraulic head data\n",
    "- The file name, directory and column headings may need to be changed\n",
    "- The units for the unit conversion may also need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_data = None\n",
    "\n",
    "if uw.mpi.rank == 0:\n",
    "\n",
    "    ti = time()\n",
    "\n",
    "    gw_data = pd.read_csv(data_dir+\"NGIS_groundwater_levels_to_2000_GAB.csv\", usecols=(0,3,4,6,7,8,9))\n",
    "\n",
    "\n",
    "    ### only use data which has a std value above 0\n",
    "#     gw_data = gw_data[gw_data['gw_level_std'] > 0.1]\n",
    "\n",
    "\n",
    "    ### remove data not within x bounds\n",
    "    gw_data = gw_data[(gw_data['easting'] > xmin) & (gw_data['easting'] < xmax)]\n",
    "    ### remove data not within y bounds\n",
    "    gw_data = gw_data[(gw_data['northing'] > ymin) & (gw_data['northing'] < ymax)]\n",
    "\n",
    "    ### remove duplicates if there are any\n",
    "    gw_data = gw_data.drop_duplicates(subset=['easting', 'northing'], keep='first')\n",
    "\n",
    "    gw_data.reset_index(inplace=True)\n",
    "\n",
    "    gw_data['matIndex'] = 0.\n",
    "\n",
    "    matIndexGWobs = []\n",
    "    gwObsPoints_data = []\n",
    "    gwObsPoints_model = []\n",
    "    pointIndex = []\n",
    "\n",
    "    topo = topo_interp((gw_data['northing'], gw_data['easting']))\n",
    "    # topo = interp_downsampled(np.c_[gw_data['easting'].values, gw_data['northing'].values])\n",
    "\n",
    "\n",
    "    for index in matIndex:\n",
    "        ### top surface\n",
    "\n",
    "        row = df_layers.loc[df_layers['mat index'] == index]\n",
    "        with rioxarray.open_rasterio(surface_filename_tiff.format(row['Layer name'].iloc[0])) as npz:\n",
    "            layer_interp_top = RegularGridInterpolator((np.flipud(npz.sel(band=1).y.data), npz.sel(band=1).x.data), np.flipud(npz.sel(band=1).data), bounds_error=False)\n",
    "        ### bottom surface\n",
    "        row = df_layers.loc[df_layers['mat index'] == index+1]\n",
    "        with rioxarray.open_rasterio(surface_filename_tiff.format(row['Layer name'].iloc[0])) as npz:\n",
    "            layer_interp_bottom = RegularGridInterpolator((np.flipud(npz.sel(band=1).y.data), npz.sel(band=1).x.data), np.flipud(npz.sel(band=1).data), bounds_error=False)\n",
    "\n",
    "\n",
    "\n",
    "        top = layer_interp_top((gw_data['northing'].values, gw_data['easting'].values))\n",
    "        bottom = layer_interp_bottom((gw_data['northing'].values, gw_data['easting'].values))\n",
    "\n",
    "#         top_dataPoints = layer_interp_top((y_gw, x_gw))\n",
    "#         bottom_dataPoints = layer_interp_bottom((y_gw, x_gw))\n",
    "\n",
    "        matIndexGWobs.append(index)\n",
    "\n",
    "\n",
    "\n",
    "        gwObsPoints_data.append((gw_data[((topo - gw_data['gw_bore_depth']) < top) & ((topo - gw_data['gw_bore_depth']) > bottom)].shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "#         gwObsPoints_model.append((gw_xyz[(z_gw < top_dataPoints) & (z_gw > bottom_dataPoints)].shape[0]))\n",
    "\n",
    "\n",
    "        ### get the index of values that should be in each layer\n",
    "        pointIndex.append((gw_data[((topo - gw_data['gw_bore_depth']) < top) & ((topo - gw_data['gw_bore_depth']) > bottom)].index.values))\n",
    "\n",
    "\n",
    "        ### add mat index to df for visualisations\n",
    "        gw_data.loc[((topo - gw_data['gw_bore_depth']) < top) & ((topo - gw_data['gw_bore_depth']) > bottom), 'matIndex'] = index\n",
    "\n",
    "    #### covert list to np array\n",
    "    pointIndex = np.asarray([y for x in pointIndex for y in x])\n",
    "\n",
    "    ### extract the points for each layer\n",
    "    gw_data = gw_data[np.isin(gw_data.index.values, pointIndex)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "gw_data = uw.mpi.comm.bcast(gw_data, root=0)\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "\n",
    "gw_boreID, gw_E, gw_N, gw_elevation, gw_depth, gw_level, gw_level_std = gw_data['ID'], gw_data['easting'].values, gw_data['northing'].values, gw_data['elevation'].values, gw_data['gw_bore_depth'].values, gw_data['gw_level'].values, gw_data['gw_level_std'].values\n",
    "\n",
    "\n",
    "gw_hydraulic_head = gw_elevation - gw_level\n",
    "gw_hydraulic_head_std = gw_level_std + 5\n",
    "gw_pressure_head = gw_depth - gw_level\n",
    "gw_pressure_head_std = gw_level_std + 5\n",
    "\n",
    "### get the initial surface elevation\n",
    "# gw_Z = interp_downsampled(np.c_[gw_E, gw_N])\n",
    "gw_Z = topo_interp((gw_N, gw_E))\n",
    "\n",
    "gw_xyz = np.c_[gw_E, gw_N, gw_Z]\n",
    "# gw_xyz, swarm_gw, index_gw = sprinkle_observations(gw_xyz, dz=10., return_swarm=True, return_index=True)\n",
    "\n",
    "#### elevation minus the bore depth\n",
    "gw_xyz[:,2] -= gw_depth\n",
    "\n",
    "### place observation points in the model\n",
    "gw_xyz0, swarm_gw0, index_gw0 = sprinkle_observations(gw_xyz, dz=10., return_swarm=True, return_index=True)\n",
    "\n",
    "topo = topo_interp((gw_data['northing'], gw_data['easting']))\n",
    "\n",
    "### only use points that are close to original points\n",
    "gw_xyz = gw_xyz[np.isclose(gw_xyz0[:,2], (topo - gw_data['gw_bore_depth']))]\n",
    "\n",
    "gw_hydraulic_head = gw_hydraulic_head[np.isclose(gw_xyz0[:,2], (topo - gw_data['gw_bore_depth']))]\n",
    "gw_hydraulic_head_std = gw_hydraulic_head_std[np.isclose(gw_xyz0[:,2], (topo - gw_data['gw_bore_depth']))]\n",
    "gw_pressure_head = gw_pressure_head[np.isclose(gw_xyz0[:,2], (topo - gw_data['gw_bore_depth']))]\n",
    "gw_pressure_head_std = gw_pressure_head_std[np.isclose(gw_xyz0[:,2], (topo - gw_data['gw_bore_depth']))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "if uw.mpi.rank == 0:\n",
    "    #### save processed gw_data file on root proc\n",
    "    gw_data = gw_data[np.isclose(gw_xyz0[:,2], (topo - gw_data['gw_bore_depth']))]\n",
    "    gw_data.to_csv(simulation_directory + 'gw_obs_data.csv')\n",
    "\n",
    "uw.mpi.comm.barrier()\n",
    "\n",
    "\n",
    "\n",
    "### place points in the model\n",
    "gw_xyz, swarm_gw, index_gw = sprinkle_observations(gw_xyz, dz=10., return_swarm=True, return_index=True)\n",
    "\n",
    "\n",
    "if uw.mpi.rank == 0 and verbose:\n",
    "    print(\"number of groundwater pressure observations = {}\".format(gw_xyz.shape[0]))\n",
    "    print(f\"Time to import pressure observations: {time()-ti} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what data you want to save during the inverse model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialgwHydraulicHead = gwHydraulicHead.data[:].copy()\n",
    "\n",
    "\n",
    "def save_ensemble(niter):\n",
    "    # gwHydraulicHead.save(simulation_directory+'checkpoints/hydraulicHeadField_{:06d}.h5'.format(niter))\n",
    "    velocityField.save(simulation_directory+'checkpoints/velocityField_{:06d}.h5'.format(niter))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define forward model\n",
    "Used to execute the model where variables are generated from the otimisation algorithm chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(x):\n",
    "    \"\"\"\n",
    "    Variables in x:\n",
    "    - k_h  : hydraulic conductivity\n",
    "    - k_t  : thermal conductivity\n",
    "    - H    : heat production\n",
    "    - Tmax : bottom temperature BC\n",
    "    \"\"\"\n",
    "    ti = time()\n",
    "\n",
    "    FM_start = time()\n",
    "\n",
    "    global niter\n",
    "\n",
    "    # check we haven't already got a solution\n",
    "    dist, idx = mintree.query(x)\n",
    "\n",
    "    if dist == 0.0 and surrogate:\n",
    "        misfit = minimiser_misfits[idx]\n",
    "        if verbose:\n",
    "            print(\"using surrogate model, misfit = {}\".format(misfit))\n",
    "        return misfit\n",
    "    else:\n",
    "        if hydraulicConductivityOnly == True:\n",
    "\n",
    "            ### scale variables\n",
    "            kh = x\n",
    "            global Tmax, kt0, H0\n",
    "            kt = kt0\n",
    "            H  = H0*1e6\n",
    "\n",
    "        else:\n",
    "        # unpack input vector\n",
    "            kh, kt, H = np.array_split(x[:-1], 3)\n",
    "            Tmax = x[-1]\n",
    "\n",
    "        ### scale variables\n",
    "        kh = 10.0**kh # log10 scale\n",
    "        H  = H*1e-6 # convert to micro\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # initialise \"default values\"\n",
    "        if hydraulicConductivityOnly == True:\n",
    "            hydraulicDiffusivity0.data[:]   = kh[-1]\n",
    "            fn_hydraulicDiffusivity.data[:] = kh[-1]\n",
    "        else:\n",
    "            hydraulicDiffusivity0.data[:]   = kh[-1]\n",
    "            fn_hydraulicDiffusivity.data[:] = kh[-1]\n",
    "\n",
    "            thermalDiffusivity0.data[:]     = kt[-1]\n",
    "            heatProduction0.data[:]         = H[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # populate swarm variables with material properties\n",
    "        if hydraulicConductivityOnly == True:\n",
    "            for i in matIndex:\n",
    "                mask_material = materialIndex0.data == i\n",
    "                hydraulicDiffusivity0.data[mask_material] = kh[i]\n",
    "\n",
    "            # determine mean of cells to produce uniform cells\n",
    "            fn_hydraulicDiffusivity.data[:,0] = np.repeat((np.mean((np.split(hydraulicDiffusivity0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "\n",
    "        else:\n",
    "            for i in matIndex:\n",
    "                mask_material = materialIndex0.data == i\n",
    "                hydraulicDiffusivity0.data[mask_material] = kh[i]\n",
    "                thermalDiffusivity0.data[mask_material]   = kt[i]\n",
    "                heatProduction0.data[mask_material]       = H[i]\n",
    "\n",
    "            # determine mean of cells to produce uniform cells\n",
    "            fn_hydraulicDiffusivity.data[:,0] = np.repeat((np.mean((np.split(hydraulicDiffusivity0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "            thermalDiffusivity.data[:,0]      = np.repeat((np.mean((np.split(thermalDiffusivity0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "            heatProduction.data[:,0]          = np.repeat((np.mean((np.split(heatProduction0.data[:,0], np.unique(swarm0.owningCell.data[:,0], return_index = True)[1])[1:]), axis=1)), GPC_solver**mesh.dim)\n",
    "\n",
    "            ### depth-dependent hydraulic conductivity\n",
    "\n",
    "            # fn_hydraulicDiffusivity.data[:] = fn_kappa(fn_hydraulicDiffusivity.data.ravel(), depth, beta).reshape(-1,1)\n",
    "\n",
    "\n",
    "        ### reset velocity and gw hydraulic head values\n",
    "        velocityField.data[:] = 1e-6\n",
    "\n",
    "        swarmVelocity.data[:] = 1e-6\n",
    "\n",
    "        gwHydraulicHead.data[:] = initialgwHydraulicHead\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print(np.isin(kh, np.unique(hydraulicDiffusivity.data)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # project HP to mesh\n",
    "        # HPproj = uw.utils.MeshVariable_Projection(heatProductionField, heatProduction, swarm)\n",
    "        # HPproj.solve()\n",
    "\n",
    "        GWsolverTimeStart = time()\n",
    "\n",
    "        ## Set up groundwater equation\n",
    "        if uw.mpi.rank == 0 and verbose:\n",
    "            print(\"Solving grounwater equation...\")\n",
    "        gwsolver.solve()\n",
    "\n",
    "        GWsolverTimeEnd = time()\n",
    "\n",
    "\n",
    "\n",
    "        ## calculate velocity from Darcy velocity\n",
    "        # velocityField.data[:] /= np.clip(fn_porosity(depth_mesh*1e-3, 0.474, 0.071, 5.989), 0.0, 1.0).reshape(-1,1)\n",
    "        velocityField.data[:] /= porosity.data[:]\n",
    "\n",
    "        # temperature-dependent conductivity\n",
    "        temperatureField.data[:] = np.clip(temperatureField.data, Tmin, Tmax)\n",
    "        temperatureField.data[topWall] = Tmin\n",
    "        temperatureField.data[bottomWall] = Tmax\n",
    "\n",
    "        # ## Set up heat equation\n",
    "        # if uw.mpi.rank == 0 and verbose:\n",
    "        #     print(\"Solving heat equation...\")\n",
    "        # for its in range(0, 20):\n",
    "        #     temperatureField0.data[:] = temperatureField.data[:]\n",
    "        #     heatsolver.solve(nonLinearIterate=False)\n",
    "        #\n",
    "        #     Tdiff = np.array(np.abs(temperatureField0.data[:] - temperatureField.data[:]).max())\n",
    "        #     Tdiff_all = np.array(0.0)\n",
    "        #     comm.Allreduce([Tdiff, MPI.DOUBLE], [Tdiff_all, MPI.DOUBLE], op=MPI.MAX)\n",
    "        #     if Tdiff_all < 0.01:\n",
    "        #         break\n",
    "\n",
    "\n",
    "        ### compare to observations and determine misfit\n",
    "\n",
    "        #         sim_dTdz = temperatureField.fn_gradient[2].evaluate(swarm_dTdz)\n",
    "        #         sim_dTdz = reduce_to_root(sim_dTdz, index_dTdz)\n",
    "        #         if uw.mpi.rank == 0:\n",
    "        #             sim_dTdz = -1.0*sim_dTdz.ravel()\n",
    "        #             misfit += (((well_dTdz - sim_dTdz)**2/0.1**2).sum())/well_dTdz.size\n",
    "        #             # print(((well_dTdz - sim_dTdz)**2/0.1**2).sum())\n",
    "\n",
    "        ### Determine velocity misfit\n",
    "        sim_vel = uw.function.math.dot(velocityField, velocityField).evaluate(swarm_recharge)\n",
    "        sim_vel = reduce_to_root(sim_vel, index_recharge)\n",
    "\n",
    "\n",
    "        ### Determine pressure misfit\n",
    "        sim_pressure_head = gwHydraulicHead.evaluate(swarm_gw) - zCoordFn.evaluate(swarm_gw)\n",
    "        sim_pressure_head = reduce_to_root(sim_pressure_head, index_gw)\n",
    "\n",
    "        def LnormMisfit(p, misfit):\n",
    "            # global recharge_vel, sim_vel, recharge_vel_std, gw_pressure_head, sim_pressure_head, gw_pressure_head_std, kh, kh0, kt, kt0, dkt, H, H0, dH\n",
    "\n",
    "            misfitType = f'L{p}-Norm'\n",
    "\n",
    "            velocity_misfit = (np.abs(np.log10(recharge_vel) - np.log10(sim_vel))**p/np.abs(np.log10(recharge_vel_std))**p).sum() #/ recharge_vel.size\n",
    "\n",
    "            misfit += velocity_misfit\n",
    "\n",
    "            pressure_misfit = (np.abs(gw_pressure_head - sim_pressure_head)**p/gw_pressure_head_std**p).sum() # / gw_pressure_head.size\n",
    "\n",
    "            misfit += pressure_misfit\n",
    "\n",
    "            ### compare hydraulic conductivity\n",
    "            HC_misfit = (np.abs(np.log10(kh) - np.log10(kh0))**p).sum()\n",
    "\n",
    "            misfit += HC_misfit\n",
    "\n",
    "\n",
    "            ### Compare thermal conductivity\n",
    "            TC_misfit = (np.abs(kt - kt0)**p/dkt**p).sum()\n",
    "\n",
    "            misfit += TC_misfit\n",
    "\n",
    "\n",
    "\n",
    "            ### Compare heat production\n",
    "            HP_misfit = (np.abs(H - H0)**p/dH**p).sum()\n",
    "\n",
    "            misfit += HP_misfit\n",
    "\n",
    "            return misfitType, misfit, velocity_misfit, pressure_misfit, HC_misfit, TC_misfit, HP_misfit\n",
    "\n",
    "\n",
    "        FM_end = time()\n",
    "\n",
    "        FM_time = FM_end - FM_start\n",
    "\n",
    "        GWTime = GWsolverTimeEnd - GWsolverTimeStart\n",
    "\n",
    "        misfit = np.array(0.0)\n",
    "        ### compare priors\n",
    "        if uw.mpi.rank == 0:\n",
    "            p_value = 2\n",
    "            misfitType, misfit, velocity_misfit, pressure_misfit, HC_misfit, TC_misfit, HP_misfit  = LnormMisfit(p=p_value, misfit=misfit)\n",
    "\n",
    "            velMisfit.append(velocity_misfit)\n",
    "            pressureMisfit.append(pressure_misfit)\n",
    "            HCMisfit.append(HC_misfit)\n",
    "            totalMisfit.append(misfit)\n",
    "            FMTime.append(FM_time)\n",
    "            GWsolverTime.append(GWTime)\n",
    "            # iteration.append(niter)\n",
    "\n",
    "            if niter < BURNIN:\n",
    "                burninPhase.append(1)\n",
    "            else:\n",
    "                burninPhase.append(0)\n",
    "\n",
    "\n",
    "            misfitData = pd.DataFrame()\n",
    "\n",
    "            misfitData['Burnin phase'] = burninPhase\n",
    "            misfitData['velMisfit'] = velMisfit\n",
    "            misfitData['pressureMisfit'] = pressureMisfit\n",
    "\n",
    "            misfitData['HCMisfit'] = HCMisfit\n",
    "            misfitData['totalMisfit'] = totalMisfit\n",
    "            misfitData['GWSolverTime'] = GWsolverTime\n",
    "            misfitData['Time'] = FMTime\n",
    "            misfitData.insert(loc=0, column='iteration', value=np.arange(0,len(FMTime)))\n",
    "\n",
    "            misfitData.to_csv(simulation_directory + str(misfitType) + '-misfitdata.csv')\n",
    "\n",
    "            if verbose == True:\n",
    "                print(f\"Misfit Type: {misfitType}\")\n",
    "                print(f'Velocity misfit: {velocity_misfit}')\n",
    "                print(f'Pressure misfit: {pressure_misfit}')\n",
    "                print(f'Hydraulic conductivity misfit: {HC_misfit}')\n",
    "                print(f'Thermal conductivity misfit: {TC_misfit}')\n",
    "                print(f'Heat production misfit: {HP_misfit}')\n",
    "\n",
    "                print(f\"Total misfit: {misfit}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        comm.Bcast([misfit, MPI.DOUBLE], root=0)\n",
    "\n",
    "\n",
    "        #### save all results to show how the burnin phase does too\n",
    "        if uw.mpi.rank == 0:\n",
    "            with open(simulation_directory+'minimiser_results.csv', 'a') as f:\n",
    "                rowwriter = csv.writer(f, delimiter=',')\n",
    "                rowwriter.writerow(np.hstack([[misfit], x]))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\\n rank {} in {:.2f} sec misfit = {}\\n\".format(uw.mpi.rank, time()-ti, misfit))\n",
    "\n",
    "        niter += 1\n",
    "\n",
    "        return misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define Metropolis Hastings model\n",
    "Used to generate the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(func, x0, nsim, burnin, bounds, x_scale, tempering=0, tune_interval=0, ):\n",
    "    \"\"\"\n",
    "    MCMC algorithm using a Metropolis-Hastings sampler.\n",
    "    Evaluates a Markov-Chain for starting values of\n",
    "    \\\\( \\\\beta, z_t, \\\\Delta z, C \\\\) and returns the\n",
    "    ensemble of model realisations.\n",
    "\n",
    "    Args:\n",
    "        func : function\n",
    "            function to generate and evaluate model, need to return a value to use in the MH algorithm\n",
    "        x0 : int\n",
    "            range of initial values used to start the MH algorithm.\n",
    "        nsim : int\n",
    "            number of simulations\n",
    "        burnin : int\n",
    "            number of burn-in simulations before to nsim\n",
    "        bounds :\n",
    "            pass list with the min and max bounds. If no bounds then pass None\n",
    "        x_scale : float\n",
    "            scaling factor for new proposals\n",
    "        tempering : float\n",
    "            used to temper the propability values\n",
    "        tune_interval :\n",
    "            used to tune the x_scale to maintain an acceptance rate of 20 - 50 %\n",
    "\n",
    "    Returns:\n",
    "        samples, acceptance, misfit_values, accept_rate, xScale\n",
    "    \"\"\"\n",
    "    x0 = np.array(x0)\n",
    "    size = len(x_scale)\n",
    "    samples = np.empty((nsim, size))\n",
    "    acceptance = np.zeros((nsim, 1))\n",
    "    accept_rate = np.empty((nsim, 1))\n",
    "    misfit_values = np.empty((nsim, 1))\n",
    "    xScale = np.empty((nsim, 1))\n",
    "\n",
    "    misfit0 = func(x0)\n",
    "    P0 = np.exp(-misfit0 / tempering)\n",
    "\n",
    "    # Burn-in phase\n",
    "    for i in range(burnin):\n",
    "        # add random perturbation\n",
    "\n",
    "        x1 = None\n",
    "\n",
    "        if uw.mpi.rank == 0:\n",
    "\n",
    "            x1 = x0 + np.random.normal(size=size) * x_scale\n",
    "            if bounds != None:\n",
    "                while not (np.all(x1 <= max(bounds)) == True and np.all(x1>=min(bounds)) == True):\n",
    "                    x1 = x0 + np.random.normal(size=size) * x_scale\n",
    "\n",
    "        uw.mpi.comm.barrier()\n",
    "\n",
    "        x1 = uw.mpi.comm.bcast(x1, root=0)\n",
    "\n",
    "        uw.mpi.comm.barrier()\n",
    "\n",
    "        # evaluate proposal probability + tempering\n",
    "        misfit1 = func(x1)\n",
    "        P1 = np.exp(-misfit1 / 10e3)\n",
    "\n",
    "        # iterate towards MAP estimate\n",
    "        if P1 > P0:\n",
    "            x0 = x1\n",
    "            P0 = P1\n",
    "            misfit0 = misfit1\n",
    "\n",
    "    # misfit0 = func(x0)\n",
    "    # P0 = np.exp(-misfit0 / 10e3)\n",
    "\n",
    "    naccept = 0\n",
    "\n",
    "    # Now sample posterior\n",
    "    for i in range(nsim):\n",
    "\n",
    "        #### tuning x_scale using scale used by pymc3\n",
    "        if tune_interval > 0:\n",
    "            if i == 0:\n",
    "                pass\n",
    "            elif tune_interval % i == 0 :\n",
    "                ### use the mean acceptance rate over last tune interval to tune the scale\n",
    "                acc_rate = np.mean(accept_rate[i-tune_interval:i])\n",
    "                    # Switch statement\n",
    "                if acc_rate < 0.001:\n",
    "                    # reduce by 90 percent\n",
    "                    x_scale = x_scale * 0.1\n",
    "                elif acc_rate < 0.05:\n",
    "                    # reduce by 50 percent\n",
    "                    x_scale  = x_scale * 0.5\n",
    "                elif acc_rate < 0.2:\n",
    "                    # reduce by ten percent\n",
    "                    x_scale  = x_scale * 0.9\n",
    "                elif acc_rate > 0.95:\n",
    "                    # increase by factor of ten\n",
    "                    x_scale  = x_scale * 10.0\n",
    "                elif acc_rate > 0.75:\n",
    "                    # increase by double\n",
    "                    x_scale  = x_scale * 2.0\n",
    "                elif acc_rate > 0.5:\n",
    "                    # increase by ten percent\n",
    "                    x_scale  = x_scale * 1.1\n",
    "\n",
    "        xScale[i] = x_scale[0]\n",
    "\n",
    "\n",
    "        # add random perturbation\n",
    "        x1 = None\n",
    "\n",
    "        if uw.mpi.rank == 0:\n",
    "\n",
    "            x1 = x0 + np.random.normal(size=size) * x_scale\n",
    "            if bounds != None:\n",
    "                while not (np.all(x1 <= max(bounds)) == True and np.all(x1>=min(bounds)) == True):\n",
    "                    x1 = x0 + np.random.normal(size=size) * x_scale\n",
    "\n",
    "        uw.mpi.comm.barrier()\n",
    "\n",
    "        x1 = uw.mpi.comm.bcast(x1, root=0)\n",
    "\n",
    "        uw.mpi.comm.barrier()\n",
    "\n",
    "        # evaluate proposal probability\n",
    "        P0 = max(P0, 1e-99)\n",
    "\n",
    "        misfit1 = func(x1)\n",
    "        P1 = np.exp(-misfit1 / tempering)\n",
    "\n",
    "        P = min(P1 / P0, 1.0)\n",
    "\n",
    "        # randomly accept probability\n",
    "        randProbability = None\n",
    "\n",
    "        if uw.mpi.rank == 0:\n",
    "\n",
    "            randProbability = np.random.rand()\n",
    "\n",
    "        uw.mpi.comm.barrier()\n",
    "\n",
    "        randProbability = uw.mpi.comm.bcast(randProbability, root=0)\n",
    "\n",
    "        uw.mpi.comm.barrier()\n",
    "\n",
    "\n",
    "        if randProbability <= P:\n",
    "        ### update values if the proposed values are accepted\n",
    "            x0 = x1\n",
    "            P0 = P1\n",
    "            misfit0 = misfit1\n",
    "            acceptance[i] = 1\n",
    "            naccept += 1\n",
    "            #### only save the accepted values\n",
    "            if n_checkpoints == 1:\n",
    "                save_ensemble(i)\n",
    "\n",
    "\n",
    "\n",
    "        misfit_values[i] = misfit0\n",
    "        samples[i] = x0\n",
    "        accept_rate[i] = naccept / (i+1)\n",
    "\n",
    "    return samples, acceptance, misfit_values, accept_rate, xScale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise x values to be used in the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hydraulicConductivityOnly == True:\n",
    "    x = np.hstack([np.log10(kh0)])\n",
    "else:\n",
    "    x = np.hstack([np.log10(kh0), kt0, H0*1e6, [Tmax]])\n",
    "\n",
    "\n",
    "\n",
    "dx = 0.01*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Initialise output table\n",
    "\n",
    "A place to store misfit and $x$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"minimiser_results.csv\" in os.listdir(simulation_directory):\n",
    "    # load existing minimiser results table\n",
    "    minimiser_results_data = np.loadtxt(simulation_directory+\"minimiser_results.csv\", delimiter=',', )\n",
    "    if not len(minimiser_results_data):\n",
    "        minimiser_results_data = np.zeros((1,x.size+1))\n",
    "    minimiser_results = minimiser_results_data[:,1:]\n",
    "    minimiser_misfits = minimiser_results_data[:,0]\n",
    "else:\n",
    "    minimiser_results = np.zeros((1,x.size))\n",
    "    minimiser_misfits = np.array([0.0])\n",
    "    if uw.mpi.rank == 0:\n",
    "        with open(simulation_directory+'minimiser_results.csv', 'w') as f:\n",
    "            pass\n",
    "\n",
    "mintree = cKDTree(minimiser_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "velMisfit      = []\n",
    "pressureMisfit = []\n",
    "totalMisfit    = []\n",
    "# iteration      = []\n",
    "HCMisfit       = []\n",
    "FMTime         = []\n",
    "GWsolverTime   = []\n",
    "burninPhase    = []\n",
    "\n",
    "\n",
    "niter = 0\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "## Check gradient of variables in the forward model\n",
    "# if hydraulicConductivityOnly == True:\n",
    "#     finite_diff_step = np.hstack([np.full_like(kh0, 1.)])\n",
    "# else:\n",
    "#     finite_diff_step = np.hstack([np.full_like(kh0, 1.), np.full_like(kt0, 0.01), np.full_like(H0, 1.), [1.0]])\n",
    "#\n",
    "# fprime_data = optimize.approx_fprime(xk=x, f=forward_model, epsilon=finite_diff_step)\n",
    "#\n",
    "# if uw.mpi.rank == 0:\n",
    "#     print(f'fprime data: {fprime_data}')\n",
    "#\n",
    "#     fprime_df = pd.DataFrame()\n",
    "#     fprime_df['fprime data'] = fprime_data\n",
    "#     fprime_df.to_csv(simulation_directory + 'fprime_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different optimisation schemes that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### differential evolution\n",
    "# res = optimize.differential_evolution(func=forward_model, bounds=bounds, args=(niter,), popsize=2, seed=42, disp=True, x0=x)\n",
    "\n",
    "\n",
    "# shgo_kwargs = dict(method='L-BFGS-B')\n",
    "\n",
    "# res = optimize.shgo(forward_model, bounds=bounds, args=(niter,), minimizer_kwargs=shgo_kwargs)\n",
    "\n",
    "# basinhopping_kwargs = dict(bounds=bounds, method = \"L-BFGS-B\", args =(niter,))\n",
    "# res = optimize.basinhopping(forward_model, x0=x, minimizer_kwargs=basinhopping_kwargs, seed=42, niter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of MH algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### metropolis hastings\n",
    "samples, acceptance, misfit_values, accept_rate, xScale = metropolis_hastings(func=forward_model, x0=x, nsim=NSIM, burnin=BURNIN, x_scale=sigma*np.ones_like(x), bounds=[-15,-3], tempering=TEMPERING, tune_interval=tune_interval, )\n",
    "\n",
    "### Save output from MH algorithm\n",
    "if uw.mpi.rank ==0:\n",
    "    MH_results = pd.DataFrame()\n",
    "    ### loop over matIndex to create a df with all the samples\n",
    "    for i in matIndex:\n",
    "        MH_results[str(i)] = samples[:,i]\n",
    "\n",
    "    MH_results['acceptance'] = acceptance\n",
    "    MH_results['acceptance rate'] = accept_rate\n",
    "    MH_results['misfit'] = misfit_values\n",
    "    MH_results['x_scale'] = xScale\n",
    "\n",
    "    ### if file exists, load in previous run and add frames together\n",
    "    if os.path.exists(simulation_directory + 'MH_output.csv'):\n",
    "        MH_results0 = pd.read_csv(simulation_directory + 'MH_output.csv')\n",
    "\n",
    "        MH_results = pd.concat(MH_results0, MH_results, ignore_index=True)\n",
    "\n",
    "    MH_results.to_csv(simulation_directory + 'MH_output.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDataStart = time()\n",
    "\n",
    "# xdmf_info_mesh  = mesh.save(simulation_directory+'mesh.h5')\n",
    "# xdmf_info_swarm = swarm.save(simulation_directory+'swarm.h5')\n",
    "\n",
    "# xdmf_info_matIndex = materialIndex.save(simulation_directory+'materialIndex.h5')\n",
    "# materialIndex.xdmf(simulation_directory+'materialIndex.xdmf', xdmf_info_matIndex, 'materialIndex', xdmf_info_swarm, 'TheSwarm')\n",
    "\n",
    "\n",
    "# # dummy mesh variable\n",
    "# phiField        = mesh.add_variable( nodeDofCount=1 )\n",
    "# heatflowField   = mesh.add_variable( nodeDofCount=3 )\n",
    "\n",
    "\n",
    "# # calculate heat flux\n",
    "# thermalDiffusivity.data[:] = fn_thermalDiffusivity.evaluate(swarm)\n",
    "# kTproj = uw.utils.MeshVariable_Projection(phiField, thermalDiffusivity, swarm)\n",
    "# kTproj.solve()\n",
    "\n",
    "# heatflowField.data[:] = temperatureField.fn_gradient.evaluate(mesh) * -phiField.data.reshape(-1,1)\n",
    "\n",
    "# rankField = mesh.add_variable( nodeDofCount=1 )\n",
    "# rankField.data[:] = uw.mpi.rank\n",
    "\n",
    "# pressureField = gwHydraulicHead.copy(deepcopy=True)\n",
    "# pressureField.data[:] -= zCoordFn.evaluate(mesh)\n",
    "\n",
    "\n",
    "# for xdmf_info,save_name,save_object in [(xdmf_info_mesh, 'velocityField', velocityField),\n",
    "#                                         (xdmf_info_mesh, 'hydraulicHeadField', gwHydraulicHead),\n",
    "#                                         (xdmf_info_mesh, 'pressureField', pressureField),\n",
    "#                                         (xdmf_info_mesh, 'temperatureField', temperatureField),\n",
    "#                                         # (xdmf_info_mesh, 'heatflowField', heatflowField),\n",
    "#                                         (xdmf_info_mesh, 'rankField', rankField),\n",
    "#                                         (xdmf_info_mesh, 'materialMesh', materialMesh),\n",
    "#                                         (xdmf_info_swarm, 'materialIndexSwarm', materialIndex),\n",
    "#                                         (xdmf_info_swarm, 'hydraulicDiffusivitySwarm', fn_hydraulicDiffusivity),\n",
    "#                                         # (xdmf_info_swarm, 'thermalDiffusivitySwarm', thermalDiffusivity),\n",
    "#                                         # (xdmf_info_swarm, 'heatProductionSwarm', heatProduction),\n",
    "#                                         ]:\n",
    "\n",
    "#     xdmf_info_var = save_object.save(simulation_directory+save_name+'.h5')\n",
    "#     save_object.xdmf(simulation_directory+save_name+'.xdmf', xdmf_info_var, save_name, xdmf_info, 'TheMesh')\n",
    "\n",
    "#     if save_name.endswith(\"Swarm\"):\n",
    "#         # project swarm variables to the mesh\n",
    "#         hydproj = uw.utils.MeshVariable_Projection(phiField, save_object, swarm)\n",
    "#         hydproj.solve()\n",
    "\n",
    "#         field_name = save_name[:-5]+'Field'\n",
    "#         xdmf_info_var = phiField.save(simulation_directory+field_name+'.h5')\n",
    "#         phiField.xdmf(simulation_directory+field_name+'.xdmf', xdmf_info_var, field_name, xdmf_info_mesh, \"TheMesh\")\n",
    "\n",
    "# # +\n",
    "# # xdmf_info_swarm_dTdz     = swarm_dTdz.save(data_dir+'swarm_dTdz.h5')\n",
    "# xdmf_info_swarm_recharge = swarm_recharge.save(simulation_directory+'swarm_recharge.h5')\n",
    "# xdmf_info_swarm_gw       = swarm_gw.save(simulation_directory+'swarm_gw.h5')\n",
    "\n",
    "# # interpolate to swarm variables (again)\n",
    "# # sim_dTdz = temperatureField.fn_gradient[2].evaluate(swarm_dTdz)\n",
    "# sim_vel = uw.function.math.dot(velocityField, velocityField).evaluate(swarm_recharge)\n",
    "# sim_pressure_head = gwHydraulicHead.evaluate(swarm_gw) - zCoordFn.evaluate(swarm_gw)\n",
    "\n",
    "\n",
    "# for save_name, this_swarm, swarm_obs, swarm_sim, index_field in [\n",
    "# #         ('dTdz', swarm_dTdz, well_dTdz, sim_dTdz, index_dTdz),\n",
    "#         ('recharge', swarm_recharge, recharge_vel, sim_vel, index_recharge),\n",
    "#         ('pressure_head', swarm_gw, gw_pressure_head, sim_pressure_head, index_gw)]:\n",
    "\n",
    "#     xdmf_info_this_swarm = this_swarm.save(simulation_directory+'swarm_{}.h5'.format(save_name))\n",
    "\n",
    "#     # save obs\n",
    "#     swarm_obs_var = this_swarm.add_variable( dataType=\"double\", count=1 )\n",
    "#     swarm_obs_var.data[:] = swarm_obs[index_field > -1].reshape(-1,1)\n",
    "#     xdmf_info_var = swarm_obs_var.save(simulation_directory+'obs_'+save_name+'.h5')\n",
    "#     swarm_obs_var.xdmf(simulation_directory+'obs_'+save_name+'.xdmf', xdmf_info_var, save_name,\n",
    "#                          xdmf_info_this_swarm, 'swarm_{}.h5'.format(save_name))\n",
    "\n",
    "#     # save sim\n",
    "#     swarm_sim_var = this_swarm.add_variable( dataType=\"double\", count=1 )\n",
    "#     swarm_sim_var.data[:] = swarm_sim.reshape(-1,1)\n",
    "#     xdmf_info_var = swarm_sim_var.save(simulation_directory+'sim_'+save_name+'.h5')\n",
    "#     swarm_sim_var.xdmf(simulation_directory+'sim_'+save_name+'.xdmf', xdmf_info_var, save_name,\n",
    "#                          xdmf_info_this_swarm, 'swarm_{}.h5'.format(save_name))\n",
    "# -\n",
    "\n",
    "# ## Save minimiser results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing data to assess model performance\n",
    "Intended to be used to determine how long an inversion will take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTiming = time()\n",
    "\n",
    "if uw.mpi.rank == 0:\n",
    "    scriptTiming = endTiming - startTiming\n",
    "    saveDataTiming = endTiming - saveDataStart\n",
    "    solverTiming = sum(FMTime) / len(FMTime)\n",
    "\n",
    "\n",
    "    timingData = np.zeros((1, 6))\n",
    "    timingData[:,0] = (Nx*Ny*Nz)\n",
    "    timingData[:,1] = (GPC_solver**3)\n",
    "    timingData[:,2] = (Nx*Ny*Nz*(GPC_solver**3))\n",
    "    timingData[:,3] = saveDataTiming\n",
    "    timingData[:,4] = solverTiming\n",
    "    timingData[:,5] = scriptTiming\n",
    "\n",
    "\n",
    "    ### save results to df in sim directory\n",
    "    timing_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    timing_data['totalCells'] = timingData[:,0]\n",
    "    timing_data['particlesPerCell'] = timingData[:,1]\n",
    "    timing_data['totalParticles'] = timingData[:,2]\n",
    "    timing_data['scriptTime'] = timingData[:,5]\n",
    "    timing_data['solverTime'] = timingData[:,4]\n",
    "    timing_data['saveDataTime'] = timingData[:,3]\n",
    "\n",
    "    timing_data.to_csv(simulation_directory + 'timing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
